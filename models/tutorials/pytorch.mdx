---
title: PyTorch
---
import { ColabLink } from '/snippets/en/_includes/colab-link.mdx';

<ColabLink url="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb" />

Use [W&B](https://wandb.ai) for machine learning experiment tracking, dataset versioning, and project collaboration.

<Frame>
    <img src="/images/tutorials/huggingface-why.png" alt="Benefits of using W&B"  />
</Frame>

[PyTorch](https://pytorch.org/) is an open-source deep learning framework known for its ease of use and Python compatibility. This tutorial shows you how to load data, define a model, and integrate your PyTorch code with W&B to add experiment tracking to your pipeline.

Follow along with a [video tutorial](https://wandb.me/pytorch-video).

<Frame>
    <img src="/images/tutorials/pytorch.png" alt="PyTorch and W&B integration diagram"  />
</Frame>

## Step 1: Install, import, and log in

Start with the following Python code to import the `wandb` library, capture a dictionary of hyperparameters with config, and start a new experiment:

```python
# import the library
import wandb

# capture a dictionary of hyperparameters with config
config = {
    "learning_rate": 0.001,
    "epochs": 100,
    "batch_size": 128
}

# start a new experiment
with wandb.init(project="new-sota-model", config=config) as run:

    # set up model and data
    model, dataloader = get_model(), get_data()

    # optional: track gradients
    run.watch(model)

    for batch in dataloader:
    metrics = model.training_step()
    # log metrics inside your training loop to visualize model performance
    run.log(metrics)

    # optional: save model at the end
    model.to_onnx()
    run.save("model.onnx")
```

Then, use the following code to import necessary modules and set random seeds and get deterministic behavior from your GPUs:

```python
import os
import random

import numpy as np
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
from tqdm.auto import tqdm

# Ensure deterministic behavior
torch.backends.cudnn.deterministic = True
random.seed(hash("setting random seeds") % 2**32 - 1)
np.random.seed(hash("improves reproducibility") % 2**32 - 1)
torch.manual_seed(hash("by removing stochasticity") % 2**32 - 1)
torch.cuda.manual_seed_all(hash("so runs are repeatable") % 2**32 - 1)

# Device configuration
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# remove slow mirror from list of MNIST mirrors
torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors
                                      if not mirror.startswith("http://yann.lecun.com")]
```

### Install the `wandb` library and log in

Use one of the following methods to install the `wandb` library and log in.

<Tabs>
<Tab title="Command Line">
1. Set the `WANDB_API_KEY` [environment variable](/models/track/environment-variables/).

    ```bash
    export WANDB_API_KEY=<your_api_key>
    ```

2. Install the `wandb` library and log in.

    ```shell
    pip install wandb
    wandb login
    ```
</Tab>
<Tab title="Python">
```bash
pip install wandb
```
```python
import wandb

wandb.login()
```
</Tab>
<Tab title="Python notebook">
```notebook
!pip install wandb
import wandb
wandb.login()
```
</Tab>
</Tabs>

## Step 2: Define the experiment and pipeline

In this step, learn how to track metadata and define the data loading and model.

### Track metadata and hyperparameters with `wandb.init`

To define the experiment, you must first define what hyperparameters and metadata to include with this run. It's standard to store this information in a `config` file or similar object and access it programatically as needed.

Create the following `config` file:

```python
config = dict(
    epochs=5,
    classes=10,
    kernels=[16, 32],
    batch_size=128,
    learning_rate=0.005,
    dataset="MNIST",
    architecture="CNN")
```

This file includes the `epochs`, `classes`, `kernels`, `batch_size`, and `learning_rate` hyperparameters. It also includes metadata, using the `MNIST` dataset and a convolutional architecture (`CNN`). Any part of your model can be defined in a `config` file like this.

Setting this metadata allows you to separate your runs if you ever work with, for example, a fully-connected architecture on CIFAR in the same project. It's a best practice to run your code in separate processes so that any issues with W&B servers don't crash your code. Once a W&B issue is resolved, you can log the data with `wandb sync`.

Next, define the overall pipeline, which has the following steps:

1. Create a model plus its associated data and optimizer using the `make` function.
2. Train the model according to your criteria using the `train` function.
3. Test the model with the `test` function.

You can do this with the following function:

```python
def model_pipeline(hyperparameters):

    # tell wandb to get started
    with wandb.init(project="pytorch-demo", config=hyperparameters) as run:
        # access all HPs through run.config, so logging matches execution.
        config = run.config

        # make the model, data, and optimization problem
        model, train_loader, test_loader, criterion, optimizer = make(config)
        print(model)

        # and use them to train the model
        train(model, train_loader, criterion, optimizer, config)

        # and test its final performance
        test(model, test_loader)

    return model
```

In this code block, you are calling the `make`, `train`, and `test` functions, which you will define later in this tutorial.

This is a standard pipeline, except it occurs within the context of `wandb.init`. Calling this function sets up a line of communication between your code and W&B servers.

Passing the `config` dictionary to `wandb.init` immediately logs all that information to the servers, so you'll always know what hyperparameter values you set your experiment to use.

To ensure the values you chose and logged are always the ones your model uses, use the `run.config` copy of your object. For example, use the following definition of `make`:

```python
def make(config):
    # Make the data
    train, test = get_data(train=True), get_data(train=False)
    train_loader = make_loader(train, batch_size=config.batch_size)
    test_loader = make_loader(test, batch_size=config.batch_size)

    # Make the model
    model = ConvNet(config.kernels, config.classes).to(device)

    # Make the loss and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(
        model.parameters(), lr=config.learning_rate)
    
    return model, train_loader, test_loader, criterion, optimizer
```

### Define the data loading and model

Next, specify how the data loads and what the model looks like:

```python
def get_data(slice=5, train=True):
    full_dataset = torchvision.datasets.MNIST(root=".",
                                              train=train, 
                                              transform=transforms.ToTensor(),
                                              download=True)
    #  equiv to slicing with [::slice] 
    sub_dataset = torch.utils.data.Subset(
      full_dataset, indices=range(0, len(full_dataset), slice))
    
    return sub_dataset


def make_loader(dataset, batch_size):
    loader = torch.utils.data.DataLoader(dataset=dataset,
                                         batch_size=batch_size, 
                                         shuffle=True,
                                         pin_memory=True, num_workers=2)
    return loader
```

Then, use the following standard ConvNet architecture or your own architecture:

```python
# Conventional and convolutional neural network

class ConvNet(nn.Module):
    def __init__(self, kernels, classes=10):
        super(ConvNet, self).__init__()
        
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.layer2 = nn.Sequential(
            nn.Conv2d(16, kernels[1], kernel_size=5, stride=1, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.fc = nn.Linear(7 * 7 * kernels[-1], classes)
        
    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.reshape(out.size(0), -1)
        out = self.fc(out)
        return out
```

## Step 3: Define training logic

Next in our `model_pipeline`, define the `train` function.

### Track gradients with `run.watch()` and everything else with `run.log()`

By calling it before you start training, the `run.watch()` `wandb` function logs your model's gradients and parameters every number of training steps equal to the `log_freq` variable.

The following training code iterates over epochs and batches, runs forward and backward passes, and applies the `optimizer`:

```python
def train(model, loader, criterion, optimizer, config):
    # Tell wandb to watch what the model gets up to: gradients, weights, and more.
    run = wandb.init(project="pytorch-demo", config=config)
    run.watch(model, criterion, log="all", log_freq=10)

    # Run training and track with wandb
    total_batches = len(loader) * config.epochs
    example_ct = 0  # number of examples seen
    batch_ct = 0
    for epoch in tqdm(range(config.epochs)):
        for _, (images, labels) in enumerate(loader):

            loss = train_batch(images, labels, model, optimizer, criterion)
            example_ct +=  len(images)
            batch_ct += 1

            # Report metrics every 25th batch
            if ((batch_ct + 1) % 25) == 0:
                train_log(loss, example_ct, epoch)


def train_batch(images, labels, model, optimizer, criterion):
    images, labels = images.to(device), labels.to(device)
    
    # Forward pass ➡
    outputs = model(images)
    loss = criterion(outputs, labels)
    
    # Backward pass ⬅
    optimizer.zero_grad()
    loss.backward()

    # Step with optimizer
    optimizer.step()

    return loss
```

The above code also calls the following `train_log` function, which uses the `run.log()` function to forward log information to B&W servers and preserve it.

```python
def train_log(loss, example_ct, epoch):
    with wandb.init(project="pytorch-demo") as run:
        # Log the loss and epoch number
        # This is where we log the metrics to W&B
        run.log({"epoch": epoch, "loss": loss}, step=example_ct)
        print(f"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}")
```

The `run.log()` function expects a dictionary with strings as keys. These strings identify the objects being logged, which make up the values. You can also optionally log which `step` of training you're on.

Optionally, you can use the number of examples the model has seen, since this makes for easier comparison across batch sizes, but you can also use raw steps or batch count. For longer training runs, it can also be useful to log by `epoch`.

## Step 4: Define testing logic

Once the model is done training, define the following `test` function and test it by running it against new data:

```python
def test(model, test_loader):
    model.eval()

    with wandb.init(project="pytorch-demo") as run:
        # Run the model on some test examples
        with torch.no_grad():
            correct, total = 0, 0
            for images, labels in test_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

            print(f"Accuracy of the model on the {total} " +
                f"test images: {correct / total:%}")
            
            run.log({"test_accuracy": correct / total})

        # Save the model in the exchangeable ONNX format
        torch.onnx.export(model, images, "model.onnx")
        run.save("model.onnx")
```

The above code also saves the model's architecture and final parameters to disk. For maximum compatibility, we recommend exporting the model in the [Open Neural Network eXchange (ONNX) format](https://onnx.ai/).

Passing that filename to `run.save()` saves the model parameters to W&B's servers, which, for example, allows you to keep track of which `.5` or `.pb` corresponds to which training runs.

For more advanced `wandb` features for storing, versioning, and distributing models, see our [Artifacts tools](https://www.wandb.com/artifacts).

## Step 5: Run training and watch your metrics live on wandb.ai

Now that you've defined the whole pipeline, you're ready to run your fully-tracked experiment. Navigate to the [**Run** page](/models/runs/#view-logged-runs) and check out these tabs:

1. **Charts**, which log the model gradients, parameter values, and loss throughout training
2. **System**, which contains a variety of system metrics, including Disk I/O utilization and CPU/GPU metrics
3. **Logs**, which displays a copy of any logs you pushed to standard out during training
4. **Files**, where, once training is complete, you can click on the `model.onnx` to view our network with the [Netron model viewer](https://github.com/lutzroeder/netron).

Once the run finishes and the `with wandb.init` block exits, you'll also see a summary of the results in the cell output.

```python
# Build, train and analyze the model with the pipeline
model = model_pipeline(config)
```

### Test hyperparameters with sweeps

This tutorial only used one set of hyperparameters. However, iterating over a number of hyperparameters is an important part of most ML workflows.

You can use W&B Sweeps to automate hyperparameter testing and explore the space of possible models and optimization strategies.

Check out a [Colab notebook demonstrating hyperparameter optimization using W&B Sweeps](https://wandb.me/sweeps-colab).

To run a hyperparameter sweep with W&B, follow these steps:

1. **Define the sweep:** Create a dictionary or a [YAML file](/models/sweeps/define-sweep-configuration/) that specifies the parameters to search through, the search strategy, the optimization metric, etc.

2. **Initialize the sweep:** Using `sweep_id = wandb.sweep(sweep_config)`.

3. **Run the sweep agent:** Using `wandb.agent(sweep_id, function=train)`.

<Frame>
    <img src="/images/tutorials/pytorch-2.png" alt="PyTorch training dashboard"  />
</Frame>

## Example gallery

For more examples of projects tracked and visualized with W&B, see our [Gallery](https://app.wandb.ai/gallery).

## Advanced setup options
1. [Environment variables](/platform/hosting/env-vars/): Set API keys in environment variables so you can run training on a managed cluster.
2. [Offline mode](/models/support/run_wandb_offline/): Use `dryrun` mode to train offline and sync results later.
3. [On-prem](/platform/hosting/hosting-options/self-managed): Install W&B in a private cloud or air-gapped servers in your own infrastructure. We have local installations for everyone from academics to enterprise teams.
4. [Sweeps](/models/sweeps/): Set up hyperparameter search quickly with our lightweight tool for tuning.
